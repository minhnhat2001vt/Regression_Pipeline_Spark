{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=f761ff96307a23501d3676ed766a7c9e3ac7a2564c113382695054e4fdca5062\n",
      "  Stored in directory: /Users/macos/Library/Caches/pip/wheels/53/fe/23/517784b9d9dadfb82c5676e76483422096aa5dc20d4d602213\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/18 00:08:51 WARN Utils: Your hostname, MacBook-Pro-8.local resolves to a loopback address: 127.0.0.1; using 172.16.16.252 instead (on interface en0)\n",
      "23/09/18 00:08:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/18 00:08:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/18 00:09:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate() # Read data from google drive\n",
    "def read_file_from_drive(filepath):\n",
    "    \"\"\"\n",
    "    filepath: path to the dataset\n",
    "    \"\"\"\n",
    "    data = spark.read.csv(filepath, inferSchema = True, header = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n"
     ]
    }
   ],
   "source": [
    "df = read_file_from_drive(\"BostonHousing.csv\")\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- crim: double (nullable = true)\n",
      " |-- zn: double (nullable = true)\n",
      " |-- indus: double (nullable = true)\n",
      " |-- chas: integer (nullable = true)\n",
      " |-- nox: double (nullable = true)\n",
      " |-- rm: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- dis: double (nullable = true)\n",
      " |-- rad: integer (nullable = true)\n",
      " |-- tax: integer (nullable = true)\n",
      " |-- ptratio: double (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- lstat: double (nullable = true)\n",
      " |-- medv: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/18 00:11:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>506</td>\n",
       "      <td>3.6135235573122535</td>\n",
       "      <td>8.601545105332491</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>88.9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>506</td>\n",
       "      <td>11.363636363636363</td>\n",
       "      <td>23.32245299451514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>506</td>\n",
       "      <td>11.136778656126504</td>\n",
       "      <td>6.860352940897589</td>\n",
       "      <td>0.46</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>506</td>\n",
       "      <td>0.0691699604743083</td>\n",
       "      <td>0.2539940413404101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>506</td>\n",
       "      <td>0.5546950592885372</td>\n",
       "      <td>0.11587767566755584</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>506</td>\n",
       "      <td>6.284634387351787</td>\n",
       "      <td>0.7026171434153232</td>\n",
       "      <td>3.561</td>\n",
       "      <td>8.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>506</td>\n",
       "      <td>68.57490118577078</td>\n",
       "      <td>28.148861406903595</td>\n",
       "      <td>2.9</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>506</td>\n",
       "      <td>3.795042687747034</td>\n",
       "      <td>2.10571012662761</td>\n",
       "      <td>1.1296</td>\n",
       "      <td>12.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>506</td>\n",
       "      <td>9.549407114624506</td>\n",
       "      <td>8.707259384239366</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>506</td>\n",
       "      <td>408.2371541501976</td>\n",
       "      <td>168.53711605495903</td>\n",
       "      <td>187</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>506</td>\n",
       "      <td>18.455533596837967</td>\n",
       "      <td>2.1649455237144455</td>\n",
       "      <td>12.6</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>506</td>\n",
       "      <td>356.67403162055257</td>\n",
       "      <td>91.29486438415782</td>\n",
       "      <td>0.32</td>\n",
       "      <td>396.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>506</td>\n",
       "      <td>12.653063241106723</td>\n",
       "      <td>7.141061511348571</td>\n",
       "      <td>1.73</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medv</th>\n",
       "      <td>506</td>\n",
       "      <td>22.532806324110698</td>\n",
       "      <td>9.197104087379815</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                   1                    2        3        4\n",
       "summary  count                mean               stddev      min      max\n",
       "crim       506  3.6135235573122535    8.601545105332491  0.00632  88.9762\n",
       "zn         506  11.363636363636363    23.32245299451514      0.0    100.0\n",
       "indus      506  11.136778656126504    6.860352940897589     0.46    27.74\n",
       "chas       506  0.0691699604743083   0.2539940413404101        0        1\n",
       "nox        506  0.5546950592885372  0.11587767566755584    0.385    0.871\n",
       "rm         506   6.284634387351787   0.7026171434153232    3.561     8.78\n",
       "age        506   68.57490118577078   28.148861406903595      2.9    100.0\n",
       "dis        506   3.795042687747034     2.10571012662761   1.1296  12.1265\n",
       "rad        506   9.549407114624506    8.707259384239366        1       24\n",
       "tax        506   408.2371541501976   168.53711605495903      187      711\n",
       "ptratio    506  18.455533596837967   2.1649455237144455     12.6     22.0\n",
       "b          506  356.67403162055257    91.29486438415782     0.32    396.9\n",
       "lstat      506  12.653063241106723    7.141061511348571     1.73    37.97\n",
       "medv       506  22.532806324110698    9.197104087379815      5.0     50.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+----+\n",
      "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|  b|lstat|medv|\n",
      "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+----+\n",
      "|   0|  0|    0|   0|  0|  0|  0|  0|  0|  0|      0|  0|    0|   0|\n",
      "+----+---+-----+----+---+---+---+---+---+---+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,lit,count,isnan,col\n",
    "\n",
    "def replace(column, value):\n",
    "    return when(column!=value,column).otherwise(lit(None))\n",
    "\n",
    "\n",
    "df.select([count(when(isnan(c)|col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "result = df.select(\"crim\",\"zn\",\"indus\",\"medv\").orderBy(\"medv\").toPandas() \n",
    "print(round(result.iloc[0,-1],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input features for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_using_pyspark(df):\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"crim\", \"zn\", \"indus\", \"chas\", \"nox\", \"rm\", \"age\", \"dis\", \"rad\", \"tax\", \"ptratio\", \"b\", \"lstat\"], \n",
    "        outputCol=\"features\")\n",
    "    data = assembler.transform(df) \n",
    "    final_data = []\n",
    "    final_data = data.select(\"features\", \"medv\")\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[features: vector, medv: double]\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data_using_pyspark(df)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42) \n",
    "def train(train_data):\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"medv\", predictionCol=\"predicted_medv\")\n",
    "    lr_model = lr.fit(train_data)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:57:55 WARN Instrumentation: [8e103150] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [-0.11362203729408954,0.048909186934053925,0.02379542898673389,2.801771998735119,-18.4154245411894,3.5158797633120065,0.0052116821614709204,-1.4163830723539739,0.3317669315937035,-0.013607893704163878,-0.9534143338408072,0.008602677392853256,-0.519503531247664]\n",
      "Intercept: 38.617\n"
     ]
    }
   ],
   "source": [
    "lr_model = train(train_data)\n",
    "coefficients = lr_model.coefficients\n",
    "intercept = lr_model.intercept\n",
    "print(\"Coefficients: \", coefficients) \n",
    "print(\"Intercept: {:.3f}\".format(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 4.672\n",
      "R-squared (R2) on test data: 0.793\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"predicted_medv\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"predicted_medv\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions) \n",
    "print(\"R-squared (R2) on test data: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important feature: nox\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"crim\", \"zn\", \"indus\", \"chas\", \"nox\", \"rm\", \"age\", \"dis\", \"rad\", \"tax\", \"ptratio\", \"b\", \"lstat\"], \n",
    "    outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "feature_importance = sorted(list(zip(data.columns[:-1], map(abs, coefficients))), key= lambda x: x[1], reverse=True)\n",
    "print(\"The most important feature:\", feature_importance[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
